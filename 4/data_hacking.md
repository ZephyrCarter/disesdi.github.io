[home](https://disesdi.github.io/) | [about](https://disesdi.github.io/about.html) | <a href="https://github.com/disesdi/" target="_blank" rel="noopener noreferrer">code</a> | [contact](https://disesdi.github.io/contact.html)


# Did An AI CEO Just Tweet About Stealing Data?

### By <a href="https://disesdi.github.io/contact.html" target="_blank" rel="noopener noreferrer">Disesdi</a> | *2023/02/28*

-------

*AI security, including the security of training data, is a real issue. Could a CEO's controversial tweet bring it to the fore?*

-------

## Introduction


Recently, Emad Mostaque (@EMostaque), the founder of [Stability AI](https://stability.ai/) –a company whose “primary drive is to generate breakthrough ideas and convert them into solutions”<sup>[1]</sup> –tweeted what appeared to some to be an endorsement of literal hacking to steal data:


![IMG_20230228_175534_810](https://user-images.githubusercontent.com/110150470/222024861-107da869-0b0d-4c16-8124-1289cc8bfd7c.jpg)


To be clear, bypassing firewalls is hacking, aka illegally accessing resources–and a crime. Data theft is a real & serious threat, so a number of people–[myself included](https://twitter.com/disesdi/status/1629349901221113857?s=20)–were concerned with a CEO who has a large following even *appearing* to advocate for stealing data.

## The Risks Of AI Data Collection

As a researcher who studies security risks associated with AIML systems, this tweet was disturbing to read (to say the least). It comes at a time when AI orgs are vulnerable, and awareness of security risks is still very low. 

In other words, data theft is currently a very real possibility with potential to impact AI development, deployment, and numerous stakeholders in the industry.

Collecting, curating, and preprocessing data for AIML applications is already both an uphill challenge, and an [ethical minefield](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/data-ethics-what-it-means-and-what-it-takes).<sup>[2]</sup> If you ask almost any data scientist or ML engineer, they’ll tell you that they almost always need/want more data. 

And the data is [running out](https://www.technologyreview.com/2022/11/24/1063684/we-could-run-out-of-data-to-train-ai-language-programs/),<sup>[3]</sup> leading [some researchers](https://arxiv.org/pdf/2211.04325.pdf) to predict we may run out of certain types of data as early as 2026.<sup>[4]</sup>

Many people have already raised the alarms about [unethical data collection](https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/),<sup>[5]</sup> the myriad means by which societal biases and other unwanted influences permeate much of our training data in [measurable ways](https://arxiv.org/abs/1608.07187),<sup>[6]</sup> and the [harms that these methodologies can cause](https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-decision-making-role/).<sup>[7]</sup> 

But so far, it’s been relatively few people with a foot in both worlds–security and AIML research–who are talking about data theft itself.

## An Increased Awareness of Data Security in AI

I’m not sure what the intent was behind this tweet. I am not a mind reader. Others indicated that in later tweets the original intent was clarified, but I read the responses and still came away feeling disturbed.

As of writing this post, the [tweet in question](https://twitter.com/emostaque/status/1629145508265570304?s=46&t=tX5qUwzcIx19K_z-_tRQNw) was still up.<sup>[8]</sup>  You can read the thread for yourself & decide what implications it may have.

Regardless of original intent, what has happened is the introduction of these ideas into more mainstream thought around AIML security. If organizations aren’t investing in the security of their data and models now–and many are not–perhaps this will be the wakeup call that they should be. 

At the very least, a conversation has been started about the ethics around what happens when the data runs out, from a security perspective.

AIML organizations would do well to listen.

-------


## [*Contact me >>*](https://disesdi.github.io/contact.html)


-------


## References

1. n.d. Stability AI. Accessed February 25, 2023. https://stability.ai/.


2. Edquist, Alex, Liz Grennan, and Sian Griffiths. 2022. “Data ethics: What it means and what it takes.” McKinsey. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/data-ethics-what-it-means-and-what-it-takes.


3. Xu, Tammy. 2022. “We could run out of data to train AI language programs.” MIT Technology Review. https://www.technologyreview.com/2022/11/24/1063684/we-could-run-out-of-data-to-train-ai-language-programs/.


4. Villalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn and An Chang Ho. “Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning.” ArXiv abs/2211.04325 (2022): n. Pag.


5. Hao, Karen. 2021. “Deleting unethical data sets isn't good enough.” MIT Technology Review. https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/.


6. Caliskan, Aylin, Joanna J. Bryson and Arvind Narayanan. “Semantics derived automatically from language corpora contain human-like biases.” Science 356 (2016): 183 - 186.


7. Pazzanese, Christina. 2020. “Ethical concerns mount as AI takes bigger decision-making role.” Harvard Gazette. https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-decision-making-role/.


8. Emad Mostaque, Twitter post, Feb 24, 2023, 7:45 a.m., https://twitter.com/emostaque/status/1629145508265570304?s=46&t=tX5qUwzcIx19K_z-_tRQNw.

-------

[home](https://disesdi.github.io/) | [about](https://disesdi.github.io/about.html) | <a href="https://github.com/disesdi/" target="_blank" rel="noopener noreferrer">code</a> | [contact](https://disesdi.github.io/contact.html)
